#!/usr/bin/env python3
"""
KEGGå›¾ç¥ç»ç½‘ç»œé¢„æµ‹è„šæœ¬
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡ŒåŒ–åˆç‰©è¿æ¥é¢„æµ‹
"""

import torch
import numpy as np
import pandas as pd
import json
from pathlib import Path
from typing import List, Tuple, Dict, Optional
import logging

from config import create_default_configs
from data_adapter import KEGGDataAdapter
from models import create_kegg_model

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class KEGGPredictor:
    """KEGGåŒ–åˆç‰©è¿æ¥é¢„æµ‹å™¨"""
    
    def __init__(self, model_path: str, data_dir: str = "kegg_real_processed"):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            data_dir: æ•°æ®ç›®å½•
        """
        self.model_path = model_path
        self.data_dir = data_dir
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½é…ç½® - ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„é…ç½®
        from config import DataConfig, ModelConfig, TrainingConfig
        
        # ä½¿ç”¨è®­ç»ƒæ—¶çš„ç®€åŒ–é…ç½®
        self.model_config = ModelConfig(
            model_type="GraphSAGE",
            input_dim=7,  # å®é™…ç‰¹å¾ç»´åº¦æ˜¯7
            hidden_dim=64,
            output_dim=32,
            num_layers=3,
            dropout=0.3
        )
        
        # åˆå§‹åŒ–æ•°æ®é€‚é…å™¨
        self.data_adapter = KEGGDataAdapter(data_dir)
        
        # åŠ è½½æ¨¡å‹
        self.model = None
        self.graph_data = None
        self._load_model()
        self._prepare_data()
    
    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        logger.info(f"ğŸ”„ åŠ è½½æ¨¡å‹: {self.model_path}")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = create_kegg_model(self.model_config)
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.model.load_state_dict(checkpoint)
        
        self.model.to(self.device)
        self.model.eval()
        
        logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _prepare_data(self):
        """å‡†å¤‡å›¾æ•°æ®"""
        logger.info("ğŸ”„ å‡†å¤‡å›¾æ•°æ®...")
        
        # åŠ è½½èŠ‚ç‚¹å’Œè¾¹æ•°æ®
        self.data_adapter.load_data()
        self.data_adapter.prepare_node_features()
        self.data_adapter.prepare_edges()
        
        # è·å–å›¾æ•°æ®
        self.graph_data = self.data_adapter.get_graph_data()
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        for key in self.graph_data:
            if isinstance(self.graph_data[key], torch.Tensor):
                self.graph_data[key] = self.graph_data[key].to(self.device)
        
        logger.info("âœ… å›¾æ•°æ®å‡†å¤‡å®Œæˆ")
        logger.info(f"   èŠ‚ç‚¹æ•°é‡: {self.graph_data['num_nodes']}")
        logger.info(f"   è¾¹æ•°é‡: {self.graph_data['edge_index'].shape[1]}")
    
    def predict_single_pair(self, compound1: str, compound2: str) -> Dict[str, float]:
        """
        é¢„æµ‹å•ä¸ªåŒ–åˆç‰©å¯¹çš„è¿æ¥æ¦‚ç‡
        
        Args:
            compound1: åŒ–åˆç‰©1çš„ID
            compound2: åŒ–åˆç‰©2çš„ID
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        # æ£€æŸ¥åŒ–åˆç‰©æ˜¯å¦å­˜åœ¨
        if compound1 not in self.data_adapter.node_mapping:
            raise ValueError(f"åŒ–åˆç‰© {compound1} ä¸åœ¨æ•°æ®é›†ä¸­")
        if compound2 not in self.data_adapter.node_mapping:
            raise ValueError(f"åŒ–åˆç‰© {compound2} ä¸åœ¨æ•°æ®é›†ä¸­")
        
        # è·å–èŠ‚ç‚¹ç´¢å¼•
        idx1 = self.data_adapter.node_mapping[compound1]
        idx2 = self.data_adapter.node_mapping[compound2]
        
        # åˆ›å»ºè¾¹å¼ é‡
        edge_tensor = torch.tensor([[idx1], [idx2]], dtype=torch.long).to(self.device)
        
        # é¢„æµ‹
        with torch.no_grad():
            logit = self.model(
                self.graph_data['x'],
                self.graph_data['edge_index'],
                edge_tensor
            )
            probability = torch.sigmoid(logit).item()
        
        return {
            'compound1': compound1,
            'compound2': compound2,
            'connection_probability': probability,
            'prediction': 'Connected' if probability > 0.5 else 'Not Connected',
            'confidence': 'High' if abs(probability - 0.5) > 0.3 else 'Medium' if abs(probability - 0.5) > 0.1 else 'Low'
        }
    
    def predict_batch_pairs(self, compound_pairs: List[Tuple[str, str]]) -> List[Dict[str, float]]:
        """
        æ‰¹é‡é¢„æµ‹åŒ–åˆç‰©å¯¹çš„è¿æ¥æ¦‚ç‡
        
        Args:
            compound_pairs: åŒ–åˆç‰©å¯¹åˆ—è¡¨
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œä¸è¾“å…¥åŒ–åˆç‰©å¯¹ä¸€ä¸€å¯¹åº”
        """
        results = []
        
        # åˆ†ç¦»æœ‰æ•ˆå’Œæ— æ•ˆçš„åŒ–åˆç‰©å¯¹
        valid_pairs = []
        valid_indices = []
        
        for i, (comp1, comp2) in enumerate(compound_pairs):
            if comp1 in self.data_adapter.node_mapping and comp2 in self.data_adapter.node_mapping:
                valid_pairs.append((comp1, comp2))
                valid_indices.append(i)
            else:
                logger.warning(f"è·³è¿‡æ— æ•ˆåŒ–åˆç‰©å¯¹: {comp1} - {comp2}")
        
        # åˆå§‹åŒ–ç»“æœåˆ—è¡¨ï¼Œä¸ºæ‰€æœ‰è¾“å…¥å¯¹é¢„ç•™ä½ç½®
        results = [None] * len(compound_pairs)
        
        # å¤„ç†æ— æ•ˆåŒ–åˆç‰©å¯¹
        for i, (comp1, comp2) in enumerate(compound_pairs):
            if i not in valid_indices:
                results[i] = {
                    'compound1': comp1,
                    'compound2': comp2,
                    'connection_probability': 0.0,
                    'prediction': 'Invalid',
                    'confidence': 'Low'
                }
        
        if not valid_pairs:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„åŒ–åˆç‰©å¯¹")
            return [r for r in results if r is not None]
        
        # åˆ›å»ºæ‰¹é‡è¾¹å¼ é‡
        indices1 = [self.data_adapter.node_mapping[pair[0]] for pair in valid_pairs]
        indices2 = [self.data_adapter.node_mapping[pair[1]] for pair in valid_pairs]
        
        edge_tensor = torch.tensor([indices1, indices2], dtype=torch.long).to(self.device)
        
        # æ‰¹é‡é¢„æµ‹
        with torch.no_grad():
            logits = self.model(
                self.graph_data['x'],
                self.graph_data['edge_index'],
                edge_tensor
            )
            probabilities = torch.sigmoid(logits).cpu().numpy()
        
        # æ•´ç†æœ‰æ•ˆåŒ–åˆç‰©å¯¹çš„ç»“æœ
        for i, (comp1, comp2) in enumerate(valid_pairs):
            # å¤„ç†å•ä¸ªé¢„æµ‹ç»“æœçš„æƒ…å†µ
            if probabilities.ndim == 0:  # æ ‡é‡æƒ…å†µ
                prob = float(probabilities)
            else:  # æ•°ç»„æƒ…å†µ
                prob = probabilities[i]
            
            # å°†ç»“æœæ”¾åˆ°æ­£ç¡®çš„ä½ç½®
            original_index = valid_indices[i]
            results[original_index] = {
                'compound1': comp1,
                'compound2': comp2,
                'connection_probability': float(prob),
                'prediction': 'Connected' if prob > 0.5 else 'Not Connected',
                'confidence': 'High' if abs(prob - 0.5) > 0.3 else 'Medium' if abs(prob - 0.5) > 0.1 else 'Low'
            }
        
        return results
    
    def find_potential_connections(self, target_compound: str, top_k: int = 10, 
                                 min_probability: float = 0.7) -> List[Dict[str, float]]:
        """
        ä¸ºæŒ‡å®šåŒ–åˆç‰©å¯»æ‰¾æ½œåœ¨çš„è¿æ¥
        
        Args:
            target_compound: ç›®æ ‡åŒ–åˆç‰©ID
            top_k: è¿”å›å‰kä¸ªæœ€å¯èƒ½çš„è¿æ¥
            min_probability: æœ€å°æ¦‚ç‡é˜ˆå€¼
            
        Returns:
            æ½œåœ¨è¿æ¥åˆ—è¡¨
        """
        if target_compound not in self.data_adapter.node_mapping:
            raise ValueError(f"åŒ–åˆç‰© {target_compound} ä¸åœ¨æ•°æ®é›†ä¸­")
        
        target_idx = self.data_adapter.node_mapping[target_compound]
        all_compounds = list(self.data_adapter.node_mapping.keys())
        
        # åˆ›å»ºæ‰€æœ‰å¯èƒ½çš„è¿æ¥å¯¹
        candidate_pairs = []
        for compound in all_compounds:
            if compound != target_compound:
                candidate_pairs.append((target_compound, compound))
        
        # æ‰¹é‡é¢„æµ‹
        logger.info(f"ğŸ”„ ä¸º {target_compound} å¯»æ‰¾æ½œåœ¨è¿æ¥...")
        results = self.predict_batch_pairs(candidate_pairs)
        
        # è¿‡æ»¤å’Œæ’åº
        filtered_results = [
            result for result in results 
            if result['connection_probability'] >= min_probability
        ]
        
        # æŒ‰æ¦‚ç‡æ’åº
        filtered_results.sort(key=lambda x: x['connection_probability'], reverse=True)
        
        return filtered_results[:top_k]
    
    def get_compound_info(self) -> pd.DataFrame:
        """è·å–æ‰€æœ‰åŒ–åˆç‰©ä¿¡æ¯"""
        return self.data_adapter.nodes_df
    
    def save_predictions(self, predictions: List[Dict], output_path: str):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        df = pd.DataFrame(predictions)
        df.to_csv(output_path, index=False)
        logger.info(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºé¢„æµ‹åŠŸèƒ½"""
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_path = "kegg_training_results/best_model.pth"
    if not Path(model_path).exists():
        logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        logger.info("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹")
        return
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = KEGGPredictor(model_path)
    
    # è·å–ä¸€äº›åŒ–åˆç‰©è¿›è¡Œæ¼”ç¤º
    compounds = list(predictor.data_adapter.node_mapping.keys())[:10]
    logger.info(f"ğŸ“‹ å¯ç”¨åŒ–åˆç‰©ç¤ºä¾‹: {compounds}")
    
    # 1. å•ä¸ªé¢„æµ‹ç¤ºä¾‹
    if len(compounds) >= 2:
        logger.info("\nğŸ¯ å•ä¸ªé¢„æµ‹ç¤ºä¾‹:")
        result = predictor.predict_single_pair(compounds[0], compounds[1])
        logger.info(f"   {result['compound1']} â†” {result['compound2']}")
        logger.info(f"   è¿æ¥æ¦‚ç‡: {result['connection_probability']:.4f}")
        logger.info(f"   é¢„æµ‹ç»“æœ: {result['prediction']}")
        logger.info(f"   ç½®ä¿¡åº¦: {result['confidence']}")
    
    # 2. æ‰¹é‡é¢„æµ‹ç¤ºä¾‹
    if len(compounds) >= 4:
        logger.info("\nğŸ“Š æ‰¹é‡é¢„æµ‹ç¤ºä¾‹:")
        test_pairs = [
            (compounds[0], compounds[1]),
            (compounds[1], compounds[2]),
            (compounds[2], compounds[3])
        ]
        
        batch_results = predictor.predict_batch_pairs(test_pairs)
        for result in batch_results:
            logger.info(f"   {result['compound1']} â†” {result['compound2']}: {result['connection_probability']:.4f}")
    
    # 3. å¯»æ‰¾æ½œåœ¨è¿æ¥ç¤ºä¾‹
    if len(compounds) >= 1:
        logger.info(f"\nğŸ” ä¸º {compounds[0]} å¯»æ‰¾æ½œåœ¨è¿æ¥:")
        potential_connections = predictor.find_potential_connections(
            compounds[0], 
            top_k=5, 
            min_probability=0.6
        )
        
        if potential_connections:
            for i, conn in enumerate(potential_connections, 1):
                logger.info(f"   {i}. {conn['compound2']}: {conn['connection_probability']:.4f} ({conn['confidence']})")
        else:
            logger.info("   æœªæ‰¾åˆ°é«˜æ¦‚ç‡çš„æ½œåœ¨è¿æ¥")
    
    logger.info("\nâœ… é¢„æµ‹æ¼”ç¤ºå®Œæˆ!")
    logger.info("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    logger.info("   from predict_kegg import KEGGPredictor")
    logger.info("   predictor = KEGGPredictor('kegg_training_results/best_model.pth')")
    logger.info("   result = predictor.predict_single_pair('compound1', 'compound2')")


if __name__ == "__main__":
    main()