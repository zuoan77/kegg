#!/usr/bin/env python3
"""
KEGGå›¾ç¥ç»ç½‘ç»œæ¨¡å‹
å®ç°ç”¨äºä»£è°¢é€šè·¯å‘ç°çš„å›¾ç¥ç»ç½‘ç»œæ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, SAGEConv, GATConv, GINConv, BatchNorm, global_mean_pool
from torch_geometric.utils import add_self_loops, degree
import math
import logging

logger = logging.getLogger(__name__)


class EnhancedEdgePredictor(nn.Module):
    """å¢å¼ºçš„è¾¹é¢„æµ‹å™¨"""
    
    def __init__(self, input_dim: int, hidden_dim: int = 128, method: str = "enhanced_concat"):
        super().__init__()
        self.method = method
        
        if method == "enhanced_concat":
            self.predictor = nn.Sequential(
                nn.Linear(input_dim * 2, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim // 2, 1)
            )
        elif method == "enhanced_hadamard":
            self.predictor = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim // 2, 1)
            )
        else:
            self.predictor = nn.Linear(input_dim * 2 if method == "concat" else input_dim, 1)
    
    def forward(self, node_embeddings: torch.Tensor, edges: torch.Tensor) -> torch.Tensor:
        """
        Args:
            node_embeddings: (num_nodes, embedding_dim)
            edges: (2, num_edges) è¾¹çš„ç´¢å¼•
        """
        src_embeddings = node_embeddings[edges[0]]  # (num_edges, embedding_dim)
        tgt_embeddings = node_embeddings[edges[1]]  # (num_edges, embedding_dim)
        
        if self.method == "concat" or self.method == "enhanced_concat":
            edge_features = torch.cat([src_embeddings, tgt_embeddings], dim=1)
        elif self.method == "hadamard" or self.method == "enhanced_hadamard":
            edge_features = src_embeddings * tgt_embeddings
        elif self.method == "cosine":
            edge_features = F.cosine_similarity(src_embeddings, tgt_embeddings, dim=1, keepdim=True)
            return edge_features.squeeze()
        else:
            raise ValueError(f"Unknown edge prediction method: {self.method}")
        
        return self.predictor(edge_features).squeeze()


class GraphSAGELayer(nn.Module):
    """è‡ªå®šä¹‰GraphSAGEå±‚"""
    
    def __init__(self, input_dim: int, output_dim: int, use_batch_norm: bool = True, 
                 use_residual: bool = True, dropout: float = 0.1):
        super().__init__()
        self.sage_conv = SAGEConv(input_dim, output_dim)
        self.use_batch_norm = use_batch_norm
        self.use_residual = use_residual and (input_dim == output_dim)
        
        if use_batch_norm:
            self.batch_norm = BatchNorm(output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
        # æ®‹å·®è¿æ¥çš„æŠ•å½±å±‚
        if use_residual and input_dim != output_dim:
            self.residual_proj = nn.Linear(input_dim, output_dim)
        else:
            self.residual_proj = None
    
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        identity = x
        
        # GraphSAGEå·ç§¯
        x = self.sage_conv(x, edge_index)
        
        # æ‰¹å½’ä¸€åŒ–
        if self.use_batch_norm:
            x = self.batch_norm(x)
        
        # æ¿€æ´»å‡½æ•°
        x = F.relu(x)
        
        # æ®‹å·®è¿æ¥
        if self.use_residual:
            if self.residual_proj is not None:
                identity = self.residual_proj(identity)
            x = x + identity
        
        # Dropout
        x = self.dropout(x)
        
        return x


class AttentionAggregator(nn.Module):
    """æ³¨æ„åŠ›èšåˆå™¨"""
    
    def __init__(self, input_dim: int, num_heads: int = 8):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = input_dim // num_heads
        
        assert input_dim % num_heads == 0, "input_dim must be divisible by num_heads"
        
        self.query = nn.Linear(input_dim, input_dim)
        self.key = nn.Linear(input_dim, input_dim)
        self.value = nn.Linear(input_dim, input_dim)
        self.output_proj = nn.Linear(input_dim, input_dim)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len, embed_dim = x.shape
        
        # è®¡ç®—Q, K, V
        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attention_weights = F.softmax(scores, dim=-1)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attended = torch.matmul(attention_weights, V)
        
        # é‡å¡‘å¹¶æŠ•å½±è¾“å‡º
        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)
        output = self.output_proj(attended)
        
        return output


class KEGGGraphModel(nn.Module):
    """KEGGå›¾ç¥ç»ç½‘ç»œæ¨¡å‹"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.model_type = config.model_type
        
        # æ„å»ºå›¾å·ç§¯å±‚
        self.conv_layers = nn.ModuleList()
        
        # è¾“å…¥å±‚
        if self.model_type == "GraphSAGE":
            self.conv_layers.append(
                GraphSAGELayer(
                    config.input_dim, 
                    config.hidden_dim,
                    config.use_batch_norm,
                    config.use_residual,
                    config.dropout
                )
            )
        elif self.model_type == "GCN":
            self.conv_layers.append(GCNConv(config.input_dim, config.hidden_dim))
        elif self.model_type == "GAT":
            self.conv_layers.append(
                GATConv(
                    config.input_dim, 
                    config.hidden_dim // config.attention_heads,
                    heads=config.attention_heads,
                    dropout=config.dropout
                )
            )
        elif self.model_type == "GIN":
            gin_nn = nn.Sequential(
                nn.Linear(config.input_dim, config.hidden_dim),
                nn.ReLU(),
                nn.Linear(config.hidden_dim, config.hidden_dim)
            )
            self.conv_layers.append(GINConv(gin_nn))
        
        # éšè—å±‚
        for i in range(config.num_layers - 2):
            if self.model_type == "GraphSAGE":
                self.conv_layers.append(
                    GraphSAGELayer(
                        config.hidden_dim,
                        config.hidden_dim,
                        config.use_batch_norm,
                        config.use_residual,
                        config.dropout
                    )
                )
            elif self.model_type == "GCN":
                self.conv_layers.append(GCNConv(config.hidden_dim, config.hidden_dim))
            elif self.model_type == "GAT":
                self.conv_layers.append(
                    GATConv(
                        config.hidden_dim,
                        config.hidden_dim // config.attention_heads,
                        heads=config.attention_heads,
                        dropout=config.dropout
                    )
                )
            elif self.model_type == "GIN":
                gin_nn = nn.Sequential(
                    nn.Linear(config.hidden_dim, config.hidden_dim),
                    nn.ReLU(),
                    nn.Linear(config.hidden_dim, config.hidden_dim)
                )
                self.conv_layers.append(GINConv(gin_nn))
        
        # è¾“å‡ºå±‚
        if config.num_layers > 1:
            if self.model_type == "GraphSAGE":
                self.conv_layers.append(
                    GraphSAGELayer(
                        config.hidden_dim,
                        config.output_dim,
                        config.use_batch_norm,
                        False,  # è¾“å‡ºå±‚ä¸ä½¿ç”¨æ®‹å·®è¿æ¥
                        config.dropout
                    )
                )
            elif self.model_type == "GCN":
                self.conv_layers.append(GCNConv(config.hidden_dim, config.output_dim))
            elif self.model_type == "GAT":
                self.conv_layers.append(
                    GATConv(
                        config.hidden_dim,
                        config.output_dim,
                        heads=1,
                        dropout=config.dropout
                    )
                )
            elif self.model_type == "GIN":
                gin_nn = nn.Sequential(
                    nn.Linear(config.hidden_dim, config.output_dim),
                    nn.ReLU(),
                    nn.Linear(config.output_dim, config.output_dim)
                )
                self.conv_layers.append(GINConv(gin_nn))
        
        # æ³¨æ„åŠ›èšåˆå™¨
        if config.use_attention:
            self.attention_aggregator = AttentionAggregator(config.output_dim, config.attention_heads)
        
        # è¾¹é¢„æµ‹å™¨
        if config.use_enhanced_predictor:
            self.edge_predictor = EnhancedEdgePredictor(
                config.output_dim,
                config.edge_pred_hidden_dim,
                config.edge_pred_method
            )
        else:
            if config.edge_pred_method == "concat":
                self.edge_predictor = nn.Linear(config.output_dim * 2, 1)
            else:
                self.edge_predictor = nn.Linear(config.output_dim, 1)
        
        # Dropoutå±‚
        self.dropout = nn.Dropout(config.dropout)
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, 
                pred_edges: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ (num_nodes, input_dim)
            edge_index: å›¾çš„è¾¹ç´¢å¼• (2, num_edges)
            pred_edges: éœ€è¦é¢„æµ‹çš„è¾¹ (2, num_pred_edges)
        
        Returns:
            edge_predictions: è¾¹é¢„æµ‹ç»“æœ (num_pred_edges,)
        """
        # å›¾å·ç§¯å±‚
        for i, conv_layer in enumerate(self.conv_layers):
            if self.model_type == "GraphSAGE":
                x = conv_layer(x, edge_index)
            else:
                x = conv_layer(x, edge_index)
                if i < len(self.conv_layers) - 1:  # ä¸åœ¨æœ€åä¸€å±‚åº”ç”¨æ¿€æ´»å‡½æ•°å’Œdropout
                    x = F.relu(x)
                    x = self.dropout(x)
        
        # æ³¨æ„åŠ›èšåˆ
        if self.config.use_attention and hasattr(self, 'attention_aggregator'):
            # ä¸ºæ³¨æ„åŠ›æœºåˆ¶é‡å¡‘å¼ é‡
            x_reshaped = x.unsqueeze(0)  # (1, num_nodes, output_dim)
            x_attended = self.attention_aggregator(x_reshaped)
            x = x_attended.squeeze(0)  # (num_nodes, output_dim)
        
        # è¾¹é¢„æµ‹
        if hasattr(self, 'edge_predictor'):
            edge_predictions = self.edge_predictor(x, pred_edges)
        else:
            # ç®€å•çš„ç‚¹ç§¯é¢„æµ‹
            src_embeddings = x[pred_edges[0]]
            tgt_embeddings = x[pred_edges[1]]
            edge_predictions = torch.sum(src_embeddings * tgt_embeddings, dim=1)
        
        return edge_predictions
    
    def get_embeddings(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        """è·å–èŠ‚ç‚¹åµŒå…¥"""
        # å›¾å·ç§¯å±‚
        for i, conv_layer in enumerate(self.conv_layers):
            if self.model_type == "GraphSAGE":
                x = conv_layer(x, edge_index)
            else:
                x = conv_layer(x, edge_index)
                if i < len(self.conv_layers) - 1:
                    x = F.relu(x)
                    x = self.dropout(x)
        
        # æ³¨æ„åŠ›èšåˆ
        if self.config.use_attention and hasattr(self, 'attention_aggregator'):
            x_reshaped = x.unsqueeze(0)
            x_attended = self.attention_aggregator(x_reshaped)
            x = x_attended.squeeze(0)
        
        return x


def create_kegg_model(config) -> KEGGGraphModel:
    """åˆ›å»ºKEGGå›¾æ¨¡å‹çš„å·¥å‚å‡½æ•°"""
    logger.info(f"ğŸ—ï¸  åˆ›å»º {config.model_type} æ¨¡å‹...")
    logger.info(f"   è¾“å…¥ç»´åº¦: {config.input_dim}")
    logger.info(f"   éšè—ç»´åº¦: {config.hidden_dim}")
    logger.info(f"   è¾“å‡ºç»´åº¦: {config.output_dim}")
    logger.info(f"   å±‚æ•°: {config.num_layers}")
    logger.info(f"   è¾¹é¢„æµ‹æ–¹æ³•: {config.edge_pred_method}")
    
    model = KEGGGraphModel(config)
    
    # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info(f"   æ€»å‚æ•°æ•°: {total_params:,}")
    logger.info(f"   å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")
    
    return model