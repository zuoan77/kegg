#!/usr/bin/env python3
"""
é“¾æ¥é¢„æµ‹è®­ç»ƒå™¨
ç”¨äºè®­ç»ƒå’Œè¯„ä¼°KEGGå›¾ç¥ç»ç½‘ç»œæ¨¡å‹
"""

import os
import time
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, precision_recall_curve
from pathlib import Path
import matplotlib.pyplot as plt
import logging
from typing import Dict, List, Tuple, Optional, Any

logger = logging.getLogger(__name__)


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    
    def __init__(self, patience: int = 50, min_delta: float = 0.001, restore_best_weights: bool = True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_score = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, score: float, model: nn.Module) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
        
        Args:
            score: å½“å‰éªŒè¯åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
            model: æ¨¡å‹å®ä¾‹
            
        Returns:
            æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ
        """
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(model)
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                if self.restore_best_weights:
                    model.load_state_dict(self.best_weights)
                return True
        else:
            self.best_score = score
            self.counter = 0
            self.save_checkpoint(model)
        
        return False
    
    def save_checkpoint(self, model: nn.Module):
        """ä¿å­˜æœ€ä½³æ¨¡å‹æƒé‡"""
        self.best_weights = model.state_dict().copy()


class MetricsCalculator:
    """æŒ‡æ ‡è®¡ç®—å™¨"""
    
    @staticmethod
    def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡"""
        metrics = {}
        
        # AUC-ROC
        try:
            metrics['auc_roc'] = roc_auc_score(y_true, y_prob)
        except ValueError:
            metrics['auc_roc'] = 0.0
        
        # AUC-PR
        try:
            metrics['auc_pr'] = average_precision_score(y_true, y_prob)
        except ValueError:
            metrics['auc_pr'] = 0.0
        
        # å‡†ç¡®ç‡
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        
        # ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
        from sklearn.metrics import precision_score, recall_score, f1_score
        
        try:
            metrics['precision'] = precision_score(y_true, y_pred, zero_division=0)
            metrics['recall'] = recall_score(y_true, y_pred, zero_division=0)
            metrics['f1'] = f1_score(y_true, y_pred, zero_division=0)
        except ValueError:
            metrics['precision'] = 0.0
            metrics['recall'] = 0.0
            metrics['f1'] = 0.0
        
        return metrics
    
    @staticmethod
    def find_best_threshold(y_true: np.ndarray, y_prob: np.ndarray) -> Tuple[float, Dict[str, float]]:
        """æ‰¾åˆ°æœ€ä½³é˜ˆå€¼"""
        precision, recall, thresholds = precision_recall_curve(y_true, y_prob)
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
        
        best_idx = np.argmax(f1_scores)
        best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5
        
        y_pred_best = (y_prob >= best_threshold).astype(int)
        best_metrics = MetricsCalculator.calculate_metrics(y_true, y_pred_best, y_prob)
        
        return best_threshold, best_metrics


class LinkPredictionTrainer:
    """é“¾æ¥é¢„æµ‹è®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, config, device: str = 'cuda'):
        self.model = model
        self.config = config
        self.device = device
        self.model.to(device)
        
        # ä¼˜åŒ–å™¨
        if config.optimizer == "Adam":
            self.optimizer = optim.Adam(
                model.parameters(),
                lr=config.learning_rate,
                weight_decay=config.weight_decay
            )
        elif config.optimizer == "SGD":
            self.optimizer = optim.SGD(
                model.parameters(),
                lr=config.learning_rate,
                weight_decay=config.weight_decay,
                momentum=0.9
            )
        else:
            raise ValueError(f"Unsupported optimizer: {config.optimizer}")
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if config.scheduler == "StepLR":
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=config.scheduler_step_size,
                gamma=config.scheduler_gamma
            )
        elif config.scheduler == "ReduceLROnPlateau":
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='max',
                factor=0.5,
                patience=20,
                verbose=True
            )
        else:
            self.scheduler = None
        
        # æŸå¤±å‡½æ•°
        if config.loss_function == "BCEWithLogitsLoss":
            pos_weight = torch.tensor([config.pos_weight]) if config.pos_weight else None
            self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        elif config.loss_function == "BCELoss":
            self.criterion = nn.BCELoss()
        else:
            raise ValueError(f"Unsupported loss function: {config.loss_function}")
        
        # æ—©åœ
        if config.early_stopping:
            self.early_stopping = EarlyStopping(
                patience=config.patience,
                min_delta=config.min_delta
            )
        else:
            self.early_stopping = None
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'train_loss': [],
            'train_auc': [],
            'val_loss': [],
            'val_auc': [],
            'val_metrics': []
        }
        
        # æœ€ä½³æ¨¡å‹
        self.best_val_auc = 0.0
        self.best_model_state = None
    
    def train_epoch(self, train_loader: DataLoader, graph_data: Dict) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        
        for batch in train_loader:
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            edges = batch['edges'].to(self.device)
            labels = batch['labels'].to(self.device)
            node_features = batch['node_features'].to(self.device)
            
            # å›¾æ•°æ®
            edge_index = graph_data['edge_index'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            predictions = self.model(node_features, edge_index, edges)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(predictions, labels.float())
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # è®°å½•
            total_loss += loss.item()
            
            # è®¡ç®—é¢„æµ‹æ¦‚ç‡
            with torch.no_grad():
                probs = torch.sigmoid(predictions)
                all_preds.extend(probs.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # è®¡ç®—å¹³å‡æŸå¤±å’ŒAUC
        avg_loss = total_loss / len(train_loader)
        
        try:
            train_auc = roc_auc_score(all_labels, all_preds)
        except ValueError:
            train_auc = 0.0
        
        return avg_loss, train_auc
    
    def validate(self, val_loader: DataLoader, graph_data: Dict) -> Tuple[float, float, Dict[str, float]]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in val_loader:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                edges = batch['edges'].to(self.device)
                labels = batch['labels'].to(self.device)
                node_features = batch['node_features'].to(self.device)
                
                # å›¾æ•°æ®
                edge_index = graph_data['edge_index'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                predictions = self.model(node_features, edge_index, edges)
                
                # è®¡ç®—æŸå¤±
                loss = self.criterion(predictions, labels.float())
                total_loss += loss.item()
                
                # è®¡ç®—é¢„æµ‹æ¦‚ç‡
                probs = torch.sigmoid(predictions)
                all_preds.extend(probs.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        avg_loss = total_loss / len(val_loader)
        
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)
        
        # æ‰¾åˆ°æœ€ä½³é˜ˆå€¼å¹¶è®¡ç®—æŒ‡æ ‡
        best_threshold, metrics = MetricsCalculator.find_best_threshold(all_labels, all_preds)
        
        return avg_loss, metrics['auc_roc'], metrics
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader, 
              graph_data: Dict) -> Dict[str, Any]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        logger.info(f"   è®­ç»ƒè½®æ•°: {self.config.epochs}")
        logger.info(f"   æ‰¹å¤§å°: {self.config.batch_size}")
        logger.info(f"   å­¦ä¹ ç‡: {self.config.learning_rate}")
        
        start_time = time.time()
        
        for epoch in range(self.config.epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_loss, train_auc = self.train_epoch(train_loader, graph_data)
            
            # éªŒè¯
            if epoch % self.config.eval_every == 0:
                val_loss, val_auc, val_metrics = self.validate(val_loader, graph_data)
                
                # è®°å½•å†å²
                self.train_history['train_loss'].append(train_loss)
                self.train_history['train_auc'].append(train_auc)
                self.train_history['val_loss'].append(val_loss)
                self.train_history['val_auc'].append(val_auc)
                self.train_history['val_metrics'].append(val_metrics)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_auc > self.best_val_auc:
                    self.best_val_auc = val_auc
                    self.best_model_state = self.model.state_dict().copy()
                
                # å­¦ä¹ ç‡è°ƒåº¦
                if self.scheduler:
                    if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                        self.scheduler.step(val_auc)
                    else:
                        self.scheduler.step()
                
                # æ—©åœæ£€æŸ¥
                if self.early_stopping:
                    if self.early_stopping(val_auc, self.model):
                        logger.info(f"æ—©åœåœ¨ç¬¬ {epoch+1} è½®")
                        break
                
                # æ‰“å°è¿›åº¦
                if self.config.verbose:
                    epoch_time = time.time() - epoch_start
                    logger.info(
                        f"Epoch {epoch+1:3d}/{self.config.epochs} | "
                        f"Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f} | "
                        f"Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f} | "
                        f"Time: {epoch_time:.2f}s"
                    )
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        if self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
        
        total_time = time.time() - start_time
        logger.info(f"âœ… è®­ç»ƒå®Œæˆ! æ€»æ—¶é—´: {total_time:.2f}s")
        logger.info(f"   æœ€ä½³éªŒè¯AUC: {self.best_val_auc:.4f}")
        
        return {
            'best_val_auc': self.best_val_auc,
            'train_history': self.train_history,
            'total_time': total_time
        }
    
    def evaluate(self, test_loader: DataLoader, graph_data: Dict) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        logger.info("ğŸ“Š è¯„ä¼°æ¨¡å‹...")
        
        self.model.eval()
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in test_loader:
                edges = batch['edges'].to(self.device)
                labels = batch['labels'].to(self.device)
                node_features = batch['node_features'].to(self.device)
                edge_index = graph_data['edge_index'].to(self.device)
                
                predictions = self.model(node_features, edge_index, edges)
                probs = torch.sigmoid(predictions)
                
                all_preds.extend(probs.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)
        
        # è®¡ç®—æŒ‡æ ‡
        best_threshold, metrics = MetricsCalculator.find_best_threshold(all_labels, all_preds)
        metrics['best_threshold'] = best_threshold
        
        logger.info("ğŸ“ˆ æµ‹è¯•ç»“æœ:")
        for metric, value in metrics.items():
            logger.info(f"   {metric}: {value:.4f}")
        
        return metrics
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'config': self.config,
            'best_val_auc': self.best_val_auc,
            'train_history': self.train_history
        }, filepath)
        logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.best_val_auc = checkpoint.get('best_val_auc', 0.0)
        self.train_history = checkpoint.get('train_history', {})
        logger.info(f"ğŸ“‚ æ¨¡å‹å·²ä» {filepath} åŠ è½½")
    
    def plot_training_curves(self, save_path: Optional[str] = None):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if not self.train_history['train_loss']:
            logger.warning("æ²¡æœ‰è®­ç»ƒå†å²æ•°æ®å¯ç»˜åˆ¶")
            return
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = range(1, len(self.train_history['train_loss']) + 1)
        
        # æŸå¤±æ›²çº¿
        ax1.plot(epochs, self.train_history['train_loss'], 'b-', label='è®­ç»ƒæŸå¤±')
        ax1.plot(epochs, self.train_history['val_loss'], 'r-', label='éªŒè¯æŸå¤±')
        ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # AUCæ›²çº¿
        ax2.plot(epochs, self.train_history['train_auc'], 'b-', label='è®­ç»ƒAUC')
        ax2.plot(epochs, self.train_history['val_auc'], 'r-', label='éªŒè¯AUC')
        ax2.set_title('è®­ç»ƒå’ŒéªŒè¯AUC')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('AUC')
        ax2.legend()
        ax2.grid(True)
        
        # éªŒè¯æŒ‡æ ‡
        if self.train_history['val_metrics']:
            precision_scores = [m.get('precision', 0) for m in self.train_history['val_metrics']]
            recall_scores = [m.get('recall', 0) for m in self.train_history['val_metrics']]
            f1_scores = [m.get('f1', 0) for m in self.train_history['val_metrics']]
            
            ax3.plot(epochs, precision_scores, 'g-', label='Precision')
            ax3.plot(epochs, recall_scores, 'orange', label='Recall')
            ax3.plot(epochs, f1_scores, 'purple', label='F1-Score')
            ax3.set_title('éªŒè¯æŒ‡æ ‡')
            ax3.set_xlabel('Epoch')
            ax3.set_ylabel('Score')
            ax3.legend()
            ax3.grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿ï¼ˆå¦‚æœæœ‰è°ƒåº¦å™¨ï¼‰
        ax4.text(0.5, 0.5, f'æœ€ä½³éªŒè¯AUC: {self.best_val_auc:.4f}', 
                ha='center', va='center', transform=ax4.transAxes, fontsize=14)
        ax4.set_title('è®­ç»ƒæ€»ç»“')
        ax4.axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.show()


def create_trainer(model, training_config, device='cpu'):
    """åˆ›å»ºè®­ç»ƒå™¨çš„å·¥å‚å‡½æ•°"""
    return LinkPredictionTrainer(
        model=model,
        config=training_config,
        device=device
    )