#!/usr/bin/env python3
"""
ä¸“é—¨å¤„ç†KEGGçœŸå®æ•°æ®çš„è„šæœ¬
å¤„ç†è¾¹æƒé‡ä¸åŒ¹é…å’Œé‡å¤è¾¹é—®é¢˜
"""

import pandas as pd
import json
import numpy as np
from pathlib import Path
import argparse

def process_kegg_data(nodes_file, edges_file, weights_file, output_dir="kegg_processed"):
    """å¤„ç†KEGGæ•°æ®ï¼Œè§£å†³è¾¹æƒé‡ä¸åŒ¹é…é—®é¢˜"""
    
    print("ğŸ”§ å¤„ç†KEGGçœŸå®æ•°æ®...")
    print("=" * 50)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # 1. è¯»å–æ•°æ®
    print("ğŸ“– è¯»å–æ•°æ®æ–‡ä»¶...")
    nodes_df = pd.read_csv(nodes_file)
    edges_df = pd.read_csv(edges_file)
    
    with open(weights_file, 'r') as f:
        weights_data = json.load(f)
    
    print(f"   èŠ‚ç‚¹æ•°: {len(nodes_df)}")
    print(f"   è¾¹æ•°: {len(edges_df)}")
    print(f"   æƒé‡æ•°: {len(weights_data)}")
    
    # 2. åˆ†æé‡å¤è¾¹
    print("\nğŸ” åˆ†æé‡å¤è¾¹...")
    edge_pairs = edges_df[['from_node', 'to_node']].copy()
    duplicates = edge_pairs.duplicated(keep=False)
    duplicate_count = duplicates.sum()
    unique_edges = len(edges_df) - duplicate_count + len(edge_pairs.drop_duplicates())
    
    print(f"   æ€»è¾¹æ•°: {len(edges_df)}")
    print(f"   é‡å¤è¾¹æ•°: {duplicate_count}")
    print(f"   å”¯ä¸€è¾¹æ•°: {unique_edges}")
    
    # 3. å¤„ç†æƒé‡åŒ¹é…
    print("\nâš–ï¸ å¤„ç†æƒé‡åŒ¹é…...")
    
    # åˆ›å»ºè¾¹åˆ°æƒé‡çš„æ˜ å°„
    edge_weights = {}
    
    # ä»æƒé‡æ–‡ä»¶ä¸­æå–æƒé‡
    for edge_key, weight_info in weights_data.items():
        if isinstance(weight_info, dict) and 'weight' in weight_info:
            weight = weight_info['weight']
        else:
            weight = 1.0  # é»˜è®¤æƒé‡
        
        # è§£æè¾¹é”®
        if ' -> ' in edge_key:
            source, target = edge_key.split(' -> ')
            edge_weights[(source, target)] = weight
    
    # 4. ä¸ºæ‰€æœ‰è¾¹åˆ†é…æƒé‡
    print("ğŸ”— ä¸ºè¾¹åˆ†é…æƒé‡...")
    processed_edges = []
    
    for _, row in edges_df.iterrows():
        source = row['from_node']
        target = row['to_node']
        
        # æŸ¥æ‰¾æƒé‡
        weight = edge_weights.get((source, target), 1.0)
        
        processed_edges.append({
            'from_node': source,
            'to_node': target,
            'weight': weight
        })
    
    processed_edges_df = pd.DataFrame(processed_edges)
    
    # 5. å¤„ç†é‡å¤è¾¹ - ä½¿ç”¨å¹³å‡æƒé‡
    print("ğŸ”„ å¤„ç†é‡å¤è¾¹ï¼ˆä½¿ç”¨å¹³å‡æƒé‡ï¼‰...")
    final_edges = processed_edges_df.groupby(['from_node', 'to_node']).agg({
        'weight': 'mean'
    }).reset_index()
    
    print(f"   å¤„ç†åè¾¹æ•°: {len(final_edges)}")
    
    # 6. åˆ›å»ºæƒé‡å­—å…¸
    final_weights = {}
    for _, row in final_edges.iterrows():
        edge_key = f"{row['from_node']} -> {row['to_node']}"
        final_weights[edge_key] = {
            'source_node': row['from_node'],
            'target_node': row['to_node'],
            'weight': float(row['weight'])
        }
    
    # 7. ä¿å­˜å¤„ç†åçš„æ•°æ®
    print("\nğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ®...")
    
    # ä¿å­˜èŠ‚ç‚¹
    nodes_output = output_path / "processed_nodes.csv"
    nodes_df.to_csv(nodes_output, index=False)
    
    # ä¿å­˜è¾¹
    edges_output = output_path / "processed_edges.csv"
    final_edges.to_csv(edges_output, index=False)
    
    # ä¿å­˜æƒé‡
    weights_output = output_path / "processed_weights.json"
    with open(weights_output, 'w') as f:
        json.dump(final_weights, f, indent=2)
    
    # ä¿å­˜å¤„ç†æŠ¥å‘Š
    report = {
        "original_data": {
            "nodes": len(nodes_df),
            "edges": len(edges_df),
            "weights": len(weights_data)
        },
        "processed_data": {
            "nodes": len(nodes_df),
            "edges": len(final_edges),
            "weights": len(final_weights)
        },
        "duplicate_edges": {
            "total_duplicates": int(duplicate_count),
            "merge_strategy": "mean"
        },
        "output_files": {
            "nodes": str(nodes_output),
            "edges": str(edges_output),
            "weights": str(weights_output)
        }
    }
    
    report_output = output_path / "processing_report.json"
    with open(report_output, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
    print(f"   è¾“å‡ºç›®å½•: {output_path}")
    print(f"   èŠ‚ç‚¹æ–‡ä»¶: {nodes_output}")
    print(f"   è¾¹æ–‡ä»¶: {edges_output}")
    print(f"   æƒé‡æ–‡ä»¶: {weights_output}")
    print(f"   æŠ¥å‘Šæ–‡ä»¶: {report_output}")
    
    return str(output_path)

def main():
    parser = argparse.ArgumentParser(description="å¤„ç†KEGGçœŸå®æ•°æ®")
    parser.add_argument("--nodes", required=False, help="èŠ‚ç‚¹æ–‡ä»¶è·¯å¾„", default="../../output/graph_builder_output/abstract_nodes_summary.csv")
    parser.add_argument("--edges", required=False, help="è¾¹æ–‡ä»¶è·¯å¾„", default="../../output/graph_builder_output/abstract_node_connections.csv")
    parser.add_argument("--weights", required=False, help="æƒé‡æ–‡ä»¶è·¯å¾„", default="../../output/graph_builder_output/edge_weights.json")
    parser.add_argument("--output", default="kegg_real_processed", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    process_kegg_data(args.nodes, args.edges, args.weights, args.output)

if __name__ == "__main__":
    main()