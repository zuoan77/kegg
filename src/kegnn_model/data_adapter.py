#!/usr/bin/env python3
"""
KEGGæ•°æ®é€‚é…å™¨
ç”¨äºå¤„ç†KEGGæ•°æ®çš„åŠ è½½ã€é¢„å¤„ç†å’Œæ‰¹å¤„ç†
"""

import os
import pandas as pd
import json
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler
import logging

logger = logging.getLogger(__name__)


class KEGGDataAdapter:
    """KEGGæ•°æ®é€‚é…å™¨ç±»"""
    
    def __init__(self, data_dir: str, normalize_features: bool = True):
        self.data_dir = Path(data_dir)
        self.normalize_features = normalize_features
        self.scaler = StandardScaler() if normalize_features else None
        
        # æ•°æ®å­˜å‚¨
        self.nodes_df = None
        self.edges_df = None
        self.weights_data = None
        self.node_features = None
        self.edge_index = None
        self.edge_weights = None
        self.node_mapping = {}
        
    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:
        """åŠ è½½KEGGæ•°æ®"""
        logger.info("ğŸ”§ åŠ è½½KEGGæ•°æ®...")
        
        # æ–‡ä»¶è·¯å¾„
        nodes_file = self.data_dir / "processed_nodes.csv"
        edges_file = self.data_dir / "processed_edges.csv"
        weights_file = self.data_dir / "processed_weights.json"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        for file_path in [nodes_file, edges_file, weights_file]:
            if not file_path.exists():
                raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è¯»å–æ•°æ®
        self.nodes_df = pd.read_csv(nodes_file)
        self.edges_df = pd.read_csv(edges_file)
        
        with open(weights_file, 'r') as f:
            self.weights_data = json.load(f)
        
        logger.info(f"   èŠ‚ç‚¹æ•°: {len(self.nodes_df)}")
        logger.info(f"   è¾¹æ•°: {len(self.edges_df)}")
        logger.info(f"   æƒé‡æ•°: {len(self.weights_data)}")
        
        return self.nodes_df, self.edges_df, self.weights_data
    
    def prepare_node_features(self) -> torch.Tensor:
        """å‡†å¤‡èŠ‚ç‚¹ç‰¹å¾"""
        if self.nodes_df is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨load_data()åŠ è½½æ•°æ®")
        
        # æå–æ•°å€¼ç‰¹å¾åˆ—
        feature_columns = []
        for col in self.nodes_df.columns:
            if col not in ['node_id', 'node_type', 'description'] and self.nodes_df[col].dtype in ['int64', 'float64']:
                feature_columns.append(col)
        
        if not feature_columns:
            # å¦‚æœæ²¡æœ‰æ•°å€¼ç‰¹å¾ï¼Œåˆ›å»ºé»˜è®¤ç‰¹å¾
            logger.warning("æœªæ‰¾åˆ°æ•°å€¼ç‰¹å¾ï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾")
            features = np.ones((len(self.nodes_df), 1))
        else:
            features = self.nodes_df[feature_columns].values
        
        # å¤„ç†ç¼ºå¤±å€¼
        features = np.nan_to_num(features, nan=0.0)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        if self.normalize_features and self.scaler is not None:
            features = self.scaler.fit_transform(features)
        
        self.node_features = torch.FloatTensor(features)
        logger.info(f"   èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {self.node_features.shape}")
        
        return self.node_features
    
    def prepare_edges(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‡†å¤‡è¾¹æ•°æ®"""
        if self.edges_df is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨load_data()åŠ è½½æ•°æ®")
        
        # åˆ›å»ºèŠ‚ç‚¹æ˜ å°„
        unique_nodes = set(self.edges_df['from_node'].tolist() + self.edges_df['to_node'].tolist())
        self.node_mapping = {node: idx for idx, node in enumerate(sorted(unique_nodes))}
        
        # è½¬æ¢è¾¹ç´¢å¼•
        source_indices = [self.node_mapping[node] for node in self.edges_df['from_node']]
        target_indices = [self.node_mapping[node] for node in self.edges_df['to_node']]
        
        # åˆ›å»ºè¾¹ç´¢å¼•å¼ é‡ (2, num_edges)
        self.edge_index = torch.LongTensor([source_indices, target_indices])
        
        # å‡†å¤‡è¾¹æƒé‡
        edge_weights = []
        for _, row in self.edges_df.iterrows():
            edge_key = f"{row['from_node']}-{row['to_node']}"
            weight = self.weights_data.get(edge_key, 1.0)
            edge_weights.append(weight)
        
        self.edge_weights = torch.FloatTensor(edge_weights)
        
        logger.info(f"   è¾¹ç´¢å¼•å½¢çŠ¶: {self.edge_index.shape}")
        logger.info(f"   è¾¹æƒé‡å½¢çŠ¶: {self.edge_weights.shape}")
        
        return self.edge_index, self.edge_weights
    
    def create_negative_edges(self, num_negative: Optional[int] = None) -> torch.Tensor:
        """åˆ›å»ºè´Ÿæ ·æœ¬è¾¹"""
        if self.edge_index is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨prepare_edges()å‡†å¤‡è¾¹æ•°æ®")
        
        num_nodes = len(self.node_mapping)
        num_positive = self.edge_index.shape[1]
        
        if num_negative is None:
            num_negative = num_positive
        
        # åˆ›å»ºæ­£è¾¹é›†åˆç”¨äºå¿«é€ŸæŸ¥æ‰¾
        positive_edges = set()
        for i in range(num_positive):
            src, tgt = self.edge_index[0, i].item(), self.edge_index[1, i].item()
            positive_edges.add((src, tgt))
            positive_edges.add((tgt, src))  # æ— å‘å›¾
        
        # ç”Ÿæˆè´Ÿæ ·æœ¬
        negative_edges = []
        attempts = 0
        max_attempts = num_negative * 10
        
        while len(negative_edges) < num_negative and attempts < max_attempts:
            src = np.random.randint(0, num_nodes)
            tgt = np.random.randint(0, num_nodes)
            
            if src != tgt and (src, tgt) not in positive_edges:
                negative_edges.append([src, tgt])
            
            attempts += 1
        
        if len(negative_edges) < num_negative:
            logger.warning(f"åªç”Ÿæˆäº† {len(negative_edges)} ä¸ªè´Ÿæ ·æœ¬ï¼Œç›®æ ‡æ˜¯ {num_negative}")
        
        return torch.LongTensor(negative_edges).t()
    
    def get_graph_data(self) -> Dict[str, torch.Tensor]:
        """è·å–å®Œæ•´çš„å›¾æ•°æ®"""
        if self.node_features is None:
            self.prepare_node_features()
        if self.edge_index is None:
            self.prepare_edges()
        
        return {
            'x': self.node_features,
            'edge_index': self.edge_index,
            'edge_weight': self.edge_weights,
            'num_nodes': len(self.node_mapping)
        }


class KEGGDataset(Dataset):
    """KEGGæ•°æ®é›†ç±»"""
    
    def __init__(self, positive_edges: torch.Tensor, negative_edges: torch.Tensor, 
                 node_features: torch.Tensor, edge_weights: Optional[torch.Tensor] = None):
        self.positive_edges = positive_edges
        self.negative_edges = negative_edges
        self.node_features = node_features
        self.edge_weights = edge_weights
        
        # åˆ›å»ºæ ‡ç­¾
        self.pos_labels = torch.ones(positive_edges.shape[1])
        self.neg_labels = torch.zeros(negative_edges.shape[1])
        
        # åˆå¹¶è¾¹å’Œæ ‡ç­¾
        self.all_edges = torch.cat([positive_edges, negative_edges], dim=1)
        self.all_labels = torch.cat([self.pos_labels, self.neg_labels])
        
        # åˆ›å»ºæƒé‡
        if edge_weights is not None:
            zero_weights = torch.zeros(negative_edges.shape[1])
            self.all_weights = torch.cat([edge_weights, zero_weights])
        else:
            self.all_weights = torch.ones(self.all_edges.shape[1])
    
    def __len__(self):
        return self.all_edges.shape[1]
    
    def __getitem__(self, idx):
        return {
            'edge': self.all_edges[:, idx],
            'label': self.all_labels[idx],
            'weight': self.all_weights[idx],
            'node_features': self.node_features
        }


def collate_kegg_batch(batch: List[Dict]) -> Dict[str, torch.Tensor]:
    """KEGGæ•°æ®æ‰¹å¤„ç†å‡½æ•°"""
    edges = torch.stack([item['edge'] for item in batch]).t()  # (2, batch_size)
    labels = torch.stack([item['label'] for item in batch])
    weights = torch.stack([item['weight'] for item in batch])
    
    # èŠ‚ç‚¹ç‰¹å¾åœ¨æ‰¹æ¬¡ä¸­æ˜¯å…±äº«çš„
    node_features = batch[0]['node_features']
    
    return {
        'edges': edges,
        'labels': labels,
        'weights': weights,
        'node_features': node_features
    }


def create_data_splits(edges: torch.Tensor, train_ratio: float = 0.7, 
                      val_ratio: float = 0.15, test_ratio: float = 0.15) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """åˆ›å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®åˆ†å‰²"""
    num_edges = edges.shape[1]
    indices = torch.randperm(num_edges)
    
    train_end = int(train_ratio * num_edges)
    val_end = train_end + int(val_ratio * num_edges)
    
    train_indices = indices[:train_end]
    val_indices = indices[train_end:val_end]
    test_indices = indices[val_end:]
    
    train_edges = edges[:, train_indices]
    val_edges = edges[:, val_indices]
    test_edges = edges[:, test_indices]
    
    return train_edges, val_edges, test_edges


def prepare_kegg_data_for_training(data_dir: str, config) -> Dict[str, Any]:
    """ä¸ºè®­ç»ƒå‡†å¤‡KEGGæ•°æ®çš„å®Œæ•´æµç¨‹"""
    logger.info("ğŸš€ å¼€å§‹å‡†å¤‡KEGGè®­ç»ƒæ•°æ®...")
    
    # åˆ›å»ºæ•°æ®é€‚é…å™¨
    adapter = KEGGDataAdapter(data_dir, normalize_features=config.normalize_features)
    
    # åŠ è½½æ•°æ®
    adapter.load_data()
    
    # å‡†å¤‡å›¾æ•°æ®
    graph_data = adapter.get_graph_data()
    
    # åˆ†å‰²è¾¹æ•°æ®
    train_edges, val_edges, test_edges = create_data_splits(
        graph_data['edge_index'], 
        config.train_ratio, 
        config.val_ratio, 
        config.test_ratio
    )
    
    # åˆ›å»ºè´Ÿæ ·æœ¬
    train_neg_edges = adapter.create_negative_edges(train_edges.shape[1])
    val_neg_edges = adapter.create_negative_edges(val_edges.shape[1])
    test_neg_edges = adapter.create_negative_edges(test_edges.shape[1])
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = KEGGDataset(train_edges, train_neg_edges, graph_data['x'])
    val_dataset = KEGGDataset(val_edges, val_neg_edges, graph_data['x'])
    test_dataset = KEGGDataset(test_edges, test_neg_edges, graph_data['x'])
    
    logger.info("âœ… KEGGæ•°æ®å‡†å¤‡å®Œæˆ")
    logger.info(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    logger.info(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    logger.info(f"   æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    
    return {
        'train_dataset': train_dataset,
        'val_dataset': val_dataset,
        'test_dataset': test_dataset,
        'graph_data': graph_data,
        'node_mapping': adapter.node_mapping
    }